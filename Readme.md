## 🧠 Telegram AI Chatbot

A lightweight Telegram chatbot powered by an **censored and safe LLaMA-based model** (via Ollama) — supports per-user chat history, customizable system prompts, and easy CLI setup.

---

## 🗂️ Project Structure

```
Telegram_AI_Chatbot-Basic-/
├── main.py                    # Entry point – run this to start the bot
├── requirements.txt           # Required Python dependencies
├── config/
│   └── config.py              # Contains the Telegram bot token
├── src/
│   ├── bot.py                 # Telegram bot logic, message handling, API communication
│   ├── model_manager.py       # Ensures the required model is installed in Ollama
│   └── prompts.py             # Loads system prompts from binary files
├── system_prompt.bin          # Optional system prompt (used with --model 69)
├── system_prompt2.bin         # Optional system prompt (used with --model 2)
└── README.md                  # You are here!
```

---

## ✅ Features

* 🚀 Chatbot with LLaMA or Mistral model backend (Ollama)
* 🧠 Maintains **chat history per Telegram user session**
* ⚙️ CLI system prompt selector (`--model` options)
* 🔐 Uses local Ollama instance — no API key required
* ✅ Safe and censored model flexibility
* ✋ `/start` to reset chat, `/stop` to shut down the bot process
* 🔄 Auto-installs model (if missing) via Ollama CLI

---

## 🧰 Requirements

* Python 3.10+
* [Ollama](https://ollama.com/) installed and running
* A Telegram bot token ([get one here](https://t.me/BotFather))
* Recommended model installed: `llama2`

---

## 🏁 Getting Started

### 1. Clone the repo and navigate in:

```bash
git clone <your-repo-url>
cd Telegram_AI_Chatbot-Basic-
```

### 2. Set up a virtual environment:

```bash
python -m venv venv
source venv/bin/activate     # On Linux/macOS
venv\Scripts\activate        # On Windows
```

### 3. Install dependencies:

```bash
pip install -r requirements.txt
```

---

## ⚙️ Configuration

### 1: Create a `.env` file in the project root

```env
# .env
TELEGRAM_BOT_TOKEN = "your-telegram-bot-token-here"
ENV_MODEL_NAME = "llama2"
```

### 2. Choose your **model** in `src/model_manager.py` (Automatically chosen, can be changed in .env file):

```python
MODEL_NAME = "llama2"
```

This model will be automatically installed via:

```bash
ollama pull llama2
```

> ℹ️ You can change to another Ollama-compatible model if needed.


## 🚦 Running the Bot

You can launch the bot with a system prompt of your choice:

```bash
python main.py --model 1   # default hardcoded prompt
python main.py --model 2   # loads from system_prompt.bin
```

---

## 💬 Telegram Commands

| Command  | Description                         |
| -------- | ----------------------------------- |
| `/start` | Start or reset the conversation     |
| `/stop`  | Stop the bot process (only locally) |

> ⚠️ `/stop` stops **only your running instance**. Others (on other devices/servers) continue if running.

---

## 🔐 Safety Notes

* You are responsible for content generated by models.
* Use a safer model like:

```python
MODEL_NAME = "llama2"
```

These chat-tuned variants are optimized for helpful, safe assistant behavior.

---

## 📦 `requirements.txt`

Ensure this includes:

```
python-telegram-bot==20.8
requests
```

(Add more as needed based on your environment.)

---

Absolutely! Here's a small, clear section you can add to your `README.md` under the **Configuration** section to explain how to use a `.env` file for storing the Telegram bot token.

---
